{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f657d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "url = 'https://search.naver.com/search.naver?ssc=tab.blog.all&sm=tab_jum&query=%EB%AA%BD%ED%82%BD+%EC%9D%BC%EC%82%B0%EB%B0%A4%EB%A6%AC%EB%8B%A8%EA%B8%B8%EC%B9%B4%ED%8E%98+%EB%B3%B8%EC%A0%90'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39ee95ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ '카피로우 일산밤리단길카페점_review_data.csv' 파일로 저장 완료\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def extract_korean(text: str) -> str:\n",
    "    \"\"\"문자열에서 한글만 추출해서 공백으로 이어붙임.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    result = re.compile('[가-힣]+').findall(text)\n",
    "    return \" \".join(result)\n",
    "\n",
    "\n",
    "def build_search_url(store_name: str) -> str:\n",
    "    \"\"\"가게 이름으로 네이버 블로그 검색 URL 생성.\"\"\"\n",
    "    query = quote_plus(store_name)  # 공백, 한글 등 인코딩\n",
    "    url = f\"https://search.naver.com/search.naver?ssc=tab.blog.all&sm=tab_jum&query={query}\"\n",
    "    return url\n",
    "\n",
    "\n",
    "def crawl_naver_blog_reviews(store_name: str, scroll_count: int = 1) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    네이버 블로그에서 특정 가게 이름으로 검색한 뒤\n",
    "    블로그 글 제목, 링크, 내용, 한글만 추출한 내용을 크롤링해서 DataFrame 반환 + CSV 저장.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    store_name : str\n",
    "        검색할 가게 이름 (예: '몽키 일산 밤리단길카페 본점')\n",
    "    scroll_count : int\n",
    "        검색 결과 화면에서 PAGE_DOWN 할 횟수 (기본 1번)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        titles, links, contents, only_kor_contents 컬럼을 가진 DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    # 1. 크롬 드라이버 실행\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "    try:\n",
    "        # 2. 검색 페이지 접속\n",
    "        search_url = build_search_url(store_name)\n",
    "        driver.get(search_url)\n",
    "        time.sleep(3)\n",
    "\n",
    "        # 3. 검색 결과 페이지 스크롤 (조금 더 로딩되게)\n",
    "        body = driver.find_element(By.CSS_SELECTOR, 'body')\n",
    "        for _ in range(scroll_count):\n",
    "            body.send_keys(Keys.PAGE_DOWN)\n",
    "            time.sleep(2)\n",
    "\n",
    "        # 4. HTML 가져와서 BeautifulSoup으로 파싱\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # 5. 검색 결과에서 블로그 제목/링크 추출 (현재 네이버 구조 기준)\n",
    "        #'div > div.sds-comps-vertical-layout.sds-comps-full-layout.N8JknjVu2Kc8aLavF6ZA > a'\n",
    "        url_soup = soup.select('div > div.sds-comps-vertical-layout.sds-comps-full-layout.N8JknjVu2Kc8aLavF6ZA > a') + soup.select('div > div.sds-comps-vertical-layout.sds-comps-full-layout.MkfloTjOr2Rg4LLlLwVA > a')\n",
    "\n",
    "        titles = []\n",
    "        links = []\n",
    "\n",
    "        for t in url_soup:\n",
    "            title_text = t.get_text().strip()\n",
    "            link = t.attrs.get('href', '').strip()\n",
    "            if not link:\n",
    "                continue\n",
    "\n",
    "            print(title_text, link)  # 디버깅용 출력\n",
    "            titles.append(title_text)\n",
    "            links.append(link)\n",
    "\n",
    "        # 6. 각 블로그 글에 들어가서 본문 크롤링\n",
    "        contents = []\n",
    "\n",
    "        for link in links:\n",
    "            try:\n",
    "                driver.get(link)\n",
    "                time.sleep(2)\n",
    "\n",
    "                # 새 에디터/구 에디터 섞여 있어서 try-except\n",
    "                try:\n",
    "                    # 구버전 블로그(iframe) 구조\n",
    "                    driver.switch_to.frame(\"mainFrame\")\n",
    "                except Exception:\n",
    "                    print('error')\n",
    "                    # iframe이 없을 수도 있으니 무시\n",
    "                    pass\n",
    "\n",
    "                try:\n",
    "                    text = driver.find_element(By.CSS_SELECTOR, 'div.se-main-container').text\n",
    "                except Exception:\n",
    "                    # 다른 구조의 블로그일 경우 대비\n",
    "                    try:\n",
    "                        text = driver.find_element(By.CSS_SELECTOR, 'div#postViewArea').text\n",
    "                    except Exception:\n",
    "                        text = \"\"\n",
    "\n",
    "                contents.append(text)\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] {link} 크롤링 중 오류: {e}\")\n",
    "                contents.append(\"\")\n",
    "\n",
    "            # 항상 frame 다시 초기화\n",
    "            driver.switch_to.default_content()\n",
    "\n",
    "        # 7. 한글만 남긴 컬럼 생성\n",
    "        only_kor_contents = [extract_korean(c) for c in contents]\n",
    "\n",
    "        # 8. DataFrame 생성\n",
    "        review_data = pd.DataFrame({\n",
    "            'titles': titles,\n",
    "            'links': links,\n",
    "            'contents': contents,\n",
    "            'only_kor_contents': only_kor_contents\n",
    "        })\n",
    "\n",
    "        # 9. 파일명에 쓸 수 없는 문자 제거 후 CSV 저장\n",
    "        safe_store_name = re.sub(r'[\\\\/*?:\"<>|]', \"_\", store_name)\n",
    "        filename = f\"{safe_store_name}_review_data.csv\"\n",
    "        review_data.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "        print(f\"✅ '{filename}' 파일로 저장 완료\")\n",
    "\n",
    "        return review_data\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "def combine_review(review_list):\n",
    "    input_string = ''\n",
    "    for i in review_list:\n",
    "        input_string += i\n",
    "    return input_string\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 예시 실행\n",
    "    store = \"카피로우 일산밤리단길카페점\"\n",
    "    df = crawl_naver_blog_reviews(store_name=store, scroll_count=1)\n",
    "    input_string = combine_review(df['only_kor_contents'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1d67159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url [<a class=\"fender-ui_228e3bd1 vnDTNtnoko3GR0aJilUy\" data-heatmap-target=\".nblg\" href=\"https://blog.naver.com/arosa0206/223327567292\" nocr=\"1\" target=\"_blank\"><span class=\"sds-comps-text sds-comps-text-ellipsis sds-comps-text-ellipsis-1 sds-comps-text-type-headline1 sds-comps-text-weight-sm\">일산 밤리단길 힐링 북카페 <mark>뒷북서재</mark></span></a>, <a class=\"fender-ui_228e3bd1 vnDTNtnoko3GR0aJilUy\" data-heatmap-target=\".nblg\" href=\"https://blog.naver.com/ageeable/223822881724\" nocr=\"1\" target=\"_blank\"><span class=\"sds-comps-text sds-comps-text-ellipsis sds-comps-text-ellipsis-1 sds-comps-text-type-headline1 sds-comps-text-weight-sm\">일산 정발산 조용한 북카페 추천 책과 만년필이 있는 <mark>뒷북서재</mark> 주차정보</span></a>, <a class=\"fender-ui_228e3bd1 vnDTNtnoko3GR0aJilUy\" data-heatmap-target=\".nblg\" href=\"https://blog.naver.com/minin0702/224033987266\" nocr=\"1\" target=\"_blank\"><span class=\"sds-comps-text sds-comps-text-ellipsis sds-comps-text-ellipsis-1 sds-comps-text-type-headline1 sds-comps-text-weight-sm\"><mark>뒷북서재</mark>ㅣ일산 밤리단길 조용한 북 카페를 찾는다면 바로 여기</span></a>, <a class=\"fender-ui_228e3bd1 vnDTNtnoko3GR0aJilUy\" data-heatmap-target=\".nblg\" href=\"https://blog.naver.com/areum9716/224097867816\" nocr=\"1\" target=\"_blank\"><span class=\"sds-comps-text sds-comps-text-ellipsis sds-comps-text-ellipsis-1 sds-comps-text-type-headline1 sds-comps-text-weight-sm\">[일산] 밤리단길 카페 추천 '<mark>뒷북 서재</mark>' 내돈내산 후기</span></a>, <a class=\"fender-ui_228e3bd1 vnDTNtnoko3GR0aJilUy\" data-heatmap-target=\".nblg\" href=\"https://blog.naver.com/traveler_dooly/223305369057\" nocr=\"1\" target=\"_blank\"><span class=\"sds-comps-text sds-comps-text-ellipsis sds-comps-text-ellipsis-1 sds-comps-text-type-headline1 sds-comps-text-weight-sm\">일산 밤리단길 편안한 북카페 <mark>뒷북서재</mark></span></a>, <a class=\"fender-ui_228e3bd1 vnDTNtnoko3GR0aJilUy\" data-heatmap-target=\".nblg\" href=\"https://blog.naver.com/go_tozzi/223736730241\" nocr=\"1\" target=\"_blank\"><span class=\"sds-comps-text sds-comps-text-ellipsis sds-comps-text-ellipsis-1 sds-comps-text-type-headline1 sds-comps-text-weight-sm\"><mark>뒷북서재</mark>::일산 밤리단길 북카페 자주 가는 최애 카페 소개</span></a>, <a class=\"fender-ui_228e3bd1 vnDTNtnoko3GR0aJilUy\" data-heatmap-target=\".nblg\" href=\"https://blog.naver.com/jy042499/223702240216\" nocr=\"1\" target=\"_blank\"><span class=\"sds-comps-text sds-comps-text-ellipsis sds-comps-text-ellipsis-1 sds-comps-text-type-headline1 sds-comps-text-weight-sm\">일산 밤리단길 조용한 공부하기 좋은 카페 북카페 <mark>뒷북서재</mark> 추천 후기</span></a>, <a class=\"fender-ui_228e3bd1 vnDTNtnoko3GR0aJilUy\" data-heatmap-target=\".nblg\" href=\"https://blog.naver.com/jm1895/223590874194\" nocr=\"1\" target=\"_blank\"><span class=\"sds-comps-text sds-comps-text-ellipsis sds-comps-text-ellipsis-1 sds-comps-text-type-headline1 sds-comps-text-weight-sm\">[책방카페] <mark>뒷북서재</mark> _밤리단길 머무르기 좋은  서점&amp;카페</span></a>, <a class=\"fender-ui_228e3bd1 vnDTNtnoko3GR0aJilUy\" data-heatmap-target=\".nblg\" href=\"https://blog.naver.com/sweetplumtea/223408248609\" nocr=\"1\" target=\"_blank\"><span class=\"sds-comps-text sds-comps-text-ellipsis sds-comps-text-ellipsis-1 sds-comps-text-type-headline1 sds-comps-text-weight-sm\"><mark>뒷북서재</mark> 후기/밤리단길 북카페 추천/서점 겸 문구점</span></a>, <a class=\"fender-ui_228e3bd1 vnDTNtnoko3GR0aJilUy\" data-heatmap-target=\".nblg\" href=\"https://blog.naver.com/doh1207/223929492241\" nocr=\"1\" target=\"_blank\"><span class=\"sds-comps-text sds-comps-text-ellipsis sds-comps-text-ellipsis-1 sds-comps-text-type-headline1 sds-comps-text-weight-sm\">[일산 카페] 책 읽으며 힐링하기 좋은 조용한 북카페, <mark>뒷북서재</mark>ㅣ내돈내먹</span></a>]\n",
      "일산 밤리단길 힐링 북카페 뒷북서재\n",
      "[BLOG] 일산 밤리단길 힐링 북카페 뒷북서재 https://blog.naver.com/arosa0206/223327567292\n",
      "일산 정발산 조용한 북카페 추천 책과 만년필이 있는 뒷북서재 주차정보\n",
      "[BLOG] 일산 정발산 조용한 북카페 추천 책과 만년필이 있는 뒷북서재 주차정보 https://blog.naver.com/ageeable/223822881724\n",
      "뒷북서재ㅣ일산 밤리단길 조용한 북 카페를 찾는다면 바로 여기\n",
      "[BLOG] 뒷북서재ㅣ일산 밤리단길 조용한 북 카페를 찾는다면 바로 여기 https://blog.naver.com/minin0702/224033987266\n",
      "[일산] 밤리단길 카페 추천 '뒷북 서재' 내돈내산 후기\n",
      "[BLOG] [일산] 밤리단길 카페 추천 '뒷북 서재' 내돈내산 후기 https://blog.naver.com/areum9716/224097867816\n",
      "일산 밤리단길 편안한 북카페 뒷북서재\n",
      "[BLOG] 일산 밤리단길 편안한 북카페 뒷북서재 https://blog.naver.com/traveler_dooly/223305369057\n",
      "[CONTENT] 1/5 글 크롤링 완료\n",
      "[CONTENT] 2/5 글 크롤링 완료\n",
      "[CONTENT] 3/5 글 크롤링 완료\n",
      "[CONTENT] 4/5 글 크롤링 완료\n",
      "[CONTENT] 5/5 글 크롤링 완료\n",
      "page title: 뒷북서재 : 네이버 블로그검색\n",
      "page length: 492572\n",
      "✅ '뒷북서재_review_data.csv' 파일로 저장 완료\n",
      "5 개의 블로그 글 크롤링 완료\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def extract_korean(text: str) -> str:\n",
    "    \"\"\"문자열에서 한글만 추출해서 공백으로 이어붙임.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    result = re.compile('[가-힣]+').findall(text)\n",
    "    return \" \".join(result)\n",
    "\n",
    "\n",
    "def build_search_url(store_name: str) -> str:\n",
    "    \"\"\"가게 이름으로 네이버 블로그 검색 URL 생성.\"\"\"\n",
    "    query = quote_plus(store_name)  # 공백, 한글 등 인코딩\n",
    "    url = f\"https://search.naver.com/search.naver?ssc=tab.blog.all&sm=tab_jum&query={query}\"\n",
    "    return url\n",
    "\n",
    "\n",
    "def _create_driver() -> webdriver.Chrome:\n",
    "    \"\"\"크롬 드라이버 생성 (속도 최적화 옵션 추가).\"\"\"\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument(\"--headless=new\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    # 이미지 로딩 끄기 (네트워크/렌더링 부담 감소)\n",
    "    prefs = {\"profile.managed_default_content_settings.images\": 2}\n",
    "    chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.set_window_size(1280, 800)\n",
    "    return driver\n",
    "\n",
    "\n",
    "def crawl_naver_blog_reviews(\n",
    "    store_name: str,\n",
    "    scroll_count: int = 1,\n",
    "    max_posts: int = 5,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    네이버 블로그에서 특정 가게 이름으로 검색한 뒤\n",
    "    블로그 글 제목, 링크, 내용, 한글만 추출한 내용을 크롤링해서 DataFrame 반환 + CSV 저장.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    store_name : str\n",
    "        검색할 가게 이름 (예: '몽키 일산 밤리단길카페 본점')\n",
    "    scroll_count : int\n",
    "        검색 결과 화면에서 PAGE_DOWN 할 횟수 (기본 1번)\n",
    "    max_posts : int\n",
    "        상위 몇 개 블로그 글만 크롤링할지 (기본 5개)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        titles, links, contents, only_kor_contents 컬럼을 가진 DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    driver = _create_driver()\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "\n",
    "    try:\n",
    "        # 1. 검색 페이지 접속\n",
    "        search_url = build_search_url(store_name)\n",
    "        driver.get(search_url)\n",
    "\n",
    "        # 2. 검색 결과 페이지 로딩 완료까지 대기\n",
    "        #    (블로그 카드 레이아웃이 뜰 때까지)\n",
    "        try:\n",
    "            wait.until(\n",
    "                EC.presence_of_all_elements_located(\n",
    "                    (By.CSS_SELECTOR,\n",
    "                     \"div.sds-comps-vertical-layout.sds-comps-full-layout\")\n",
    "                )\n",
    "            )\n",
    "        except Exception:\n",
    "            # 그래도 못 찾으면 그냥 현재 페이지 기준으로 진행\n",
    "            pass\n",
    "\n",
    "        # 3. 스크롤 (조금만)\n",
    "        body = driver.find_element(By.CSS_SELECTOR, \"body\")\n",
    "        for _ in range(scroll_count):\n",
    "            body.send_keys(Keys.PAGE_DOWN)\n",
    "            # 너무 오래 기다릴 필요 없이 짧게\n",
    "            time.sleep(0.5)\n",
    "\n",
    "        # 4. HTML 가져와서 BeautifulSoup으로 파싱\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        # 5. 검색 결과에서 블로그 제목/링크 추출\n",
    "        url_soup = (\n",
    "            soup.select(\n",
    "                \"div > div.sds-comps-vertical-layout.sds-comps-full-layout.N8JknjVu2Kc8aLavF6ZA > a\"\n",
    "            )\n",
    "            + soup.select(\n",
    "                \"div > div.sds-comps-vertical-layout.sds-comps-full-layout.MkfloTjOr2Rg4LLlLwVA > a\"\n",
    "            )\n",
    "            +soup.select(\n",
    "                'div > div.sds-comps-vertical-layout.sds-comps-full-layout._IJWW1BBVoZsf1hqzdtq > a'\n",
    "                )\n",
    "        )\n",
    "        print('url', url_soup)\n",
    "        # 상위 max_posts개만 사용\n",
    "        url_soup = url_soup[:max_posts]\n",
    "\n",
    "        titles = []\n",
    "        links = []\n",
    "\n",
    "        for t in url_soup:\n",
    "            title_text = t.get_text().strip()\n",
    "            print(title_text)\n",
    "            link = t.attrs.get(\"href\", \"\").strip()\n",
    "            if not link:\n",
    "                continue\n",
    "\n",
    "            print(\"[BLOG]\", title_text, link)  # 디버깅용 출력\n",
    "            titles.append(title_text)\n",
    "            links.append(link)\n",
    "\n",
    "        # 6. 각 블로그 글에 들어가서 본문 크롤링\n",
    "        contents = []\n",
    "\n",
    "        for idx, link in enumerate(links):\n",
    "            try:\n",
    "                driver.get(link)\n",
    "\n",
    "                # 블로그 본문 로딩 기다리기\n",
    "                # 새 에디터\n",
    "                try:\n",
    "                    wait.until(\n",
    "                        EC.presence_of_element_located(\n",
    "                            (By.CSS_SELECTOR, \"iframe#mainFrame, div.se-main-container, div#postViewArea\")\n",
    "                        )\n",
    "                    )\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "                # 구버전 블로그(iframe) 구조일 경우 frame 전환\n",
    "                switched_to_frame = False\n",
    "                try:\n",
    "                    driver.switch_to.frame(\"mainFrame\")\n",
    "                    switched_to_frame = True\n",
    "                except Exception:\n",
    "                    # iframe이 없을 수도 있으니 무시\n",
    "                    pass\n",
    "\n",
    "                text = \"\"\n",
    "                # 새 에디터\n",
    "                try:\n",
    "                    text = driver.find_element(\n",
    "                        By.CSS_SELECTOR, \"div.se-main-container\"\n",
    "                    ).text\n",
    "                except Exception:\n",
    "                    # 구 에디터\n",
    "                    try:\n",
    "                        text = driver.find_element(\n",
    "                            By.CSS_SELECTOR, \"div#postViewArea\"\n",
    "                        ).text\n",
    "                    except Exception:\n",
    "                        text = \"\"\n",
    "\n",
    "                contents.append(text)\n",
    "\n",
    "                # 항상 frame 다시 초기화\n",
    "                if switched_to_frame:\n",
    "                    driver.switch_to.default_content()\n",
    "\n",
    "                print(f\"[CONTENT] {idx+1}/{len(links)} 글 크롤링 완료\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] {link} 크롤링 중 오류: {e}\")\n",
    "                contents.append(\"\")\n",
    "                try:\n",
    "                    driver.switch_to.default_content()\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "\n",
    "        driver.get(search_url)\n",
    "        time.sleep(3)\n",
    "\n",
    "        print(\"page title:\", driver.title)\n",
    "        print(\"page length:\", len(driver.page_source))\n",
    "\n",
    "        # 7. 한글만 남긴 컬럼 생성\n",
    "        only_kor_contents = [extract_korean(c) for c in contents]\n",
    "\n",
    "        # 8. DataFrame 생성\n",
    "        review_data = pd.DataFrame(\n",
    "            {\n",
    "                \"titles\": titles,\n",
    "                \"links\": links,\n",
    "                \"contents\": contents,\n",
    "                \"only_kor_contents\": only_kor_contents,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # 9. 파일명에 쓸 수 없는 문자 제거 후 CSV 저장\n",
    "        safe_store_name = re.sub(r'[\\\\/*?:\"<>|]', \"_\", store_name)\n",
    "        filename = f\"{safe_store_name}_review_data.csv\"\n",
    "        review_data.to_csv(filename, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"✅ '{filename}' 파일로 저장 완료\")\n",
    "\n",
    "        return review_data\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "def combine_review(review_list):\n",
    "    input_string = \"\"\n",
    "    for i in review_list:\n",
    "        input_string += i\n",
    "    return input_string\n",
    "\n",
    "\n",
    "def build_review_input_text(review_series, max_chars_per_review=2000):\n",
    "    texts = []\n",
    "    for raw in review_series:\n",
    "        if not isinstance(raw, str):\n",
    "            continue\n",
    "        text = raw.strip()\n",
    "        if not text:\n",
    "            continue\n",
    "        # 너무 긴 리뷰는 앞부분만 사용\n",
    "        text = text[:max_chars_per_review]\n",
    "        texts.append(text)\n",
    "\n",
    "    # 리뷰들 사이에 구분선을 넣어서 합치기\n",
    "    return \"\\n\\n---\\n\\n\".join(texts)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 예시 실행\n",
    "    store = \"뒷북서재\"\n",
    "    df = crawl_naver_blog_reviews(store_name=store, scroll_count=1, max_posts=5)\n",
    "    input_string = build_review_input_text(df[\"only_kor_contents\"])\n",
    "    print(len(df), \"개의 블로그 글 크롤링 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6991850c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7497"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc954ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6116"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d12ed814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_review(review_list):\n",
    "    input_string = ''\n",
    "    for i in review_list:\n",
    "        input_string += i\n",
    "    return input_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dfecfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "디저트가 특히 맛있었던 일산 카페, 카피로우 일산밤리단길카페점 방문기 https://blog.naver.com/hidew777/224076747828\n",
      "카피로우 일산밤리단길카페점 솔직후기 https://blog.naver.com/rhdmsdl0720/223996105928\n",
      "카피로우 일산밤리단길카페점 내돈내산 솔직후기 https://blog.naver.com/rkguscjswo26/223891656310\n",
      "“카피로우” 일산밤리단길카페점.크렘당쥬?프랑스디저트 천사의크림일산 밤리단길카페추천 https://blog.naver.com/comma2024/223785598110\n",
      "[경기/고양시] 일산동구 마두동 밤리단길 데이트 카페 추천 무화과 디저트 맛집 찾고 있다면? ‘카피로우 일산밤리단길카페점’ - 아메리카노, 무화과요거트케이크(글루텐프리) https://blog.naver.com/vivid_07/224064134098\n",
      "일산 카페 찐맛집, 카피로우 일산밤리단길카페점 https://blog.naver.com/soheee666/223960234321\n",
      "밤리단길 디저트 맛집 카피로우 느좋 테라스 카페 https://blog.naver.com/enterside/223918465299\n",
      "일산 밤리단길 | 카피로우 https://blog.naver.com/rafig0913/223881827603\n",
      "✅ '카피로우 일산밤리단길카페점_review_data.csv' 파일로 저장 완료\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'only_kor_review'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hisem\\Desktop\\pipeline\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'only_kor_review'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m store = \u001b[33m\"\u001b[39m\u001b[33m카피로우 일산밤리단길카페점\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m df = crawl_naver_blog_reviews(store_name=store, scroll_count=\u001b[32m1\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m input_string = combine_review(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43monly_kor_review\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hisem\\Desktop\\pipeline\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hisem\\Desktop\\pipeline\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'only_kor_review'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 예시 실행\n",
    "    store = \"카피로우 일산밤리단길카페점\"\n",
    "    df = crawl_naver_blog_reviews(store_name=store, scroll_count=1)\n",
    "    input_string = combine_review(df['only_kor_contents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "494b53d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_string = combine_review(df['only_kor_contents'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa5e4e5",
   "metadata": {},
   "source": [
    "# Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0b2d58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main_menu=['카페라떼', '땅콩크림라떼', '단호박케이크', '무화과요거트케이크'] atmosphere=['따뜻하고 아늑한 분위기', '화이트톤 우드 인테리어', '조용하고 편안한 공간', '반려견 동반 가능'] recommended_for=['연인', '친구', '반려견과 함께 방문', '디저트/케이크 애호가']\n",
      "['카페라떼', '땅콩크림라떼', '단호박케이크', '무화과요거트케이크']\n",
      "['따뜻하고 아늑한 분위기', '화이트톤 우드 인테리어', '조용하고 편안한 공간', '반려견 동반 가능']\n",
      "['연인', '친구', '반려견과 함께 방문', '디저트/케이크 애호가']\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# --- 1) 구조화된 Output 모델 ---\n",
    "class ReviewExtraction(BaseModel):\n",
    "    main_menu: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"가게에서 많이 언급되는 대표 메뉴 키워드들 (예: 소금빵, 아메리카노, 고구마라떼)\"\n",
    "    )\n",
    "    atmosphere: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"가게 분위기, 경험, 매장 특징 키워드들 (예: 아늑한, 감성적인, 좌석이 넓은)\"\n",
    "    )\n",
    "    recommended_for: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"어떤 유형의 사람이 방문하면 좋은지 (예: 연인과 함께, 친구와 수다, 반려견과 함께)\"\n",
    "    )\n",
    "\n",
    "\n",
    "# --- 2) LLM + 구조화 출력 준비 ---\n",
    "model = ChatOpenAI(\n",
    "    model_name=\"gpt-5-nano\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    temperature=0.2\n",
    ").with_structured_output(ReviewExtraction)\n",
    "\n",
    "\n",
    "# --- 3) 프롬프트 ---\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "너는 한국어 네이버 블로그 리뷰를 분석해서\n",
    "가게의 대표 메뉴, 분위기, 추천 대상을 키워드로만 뽑는 역할을 한다.\n",
    "\n",
    "아래 리뷰 텍스트를 보고,\n",
    "각 항목당 3~4개의 핵심 키워드를 한국어로만 추출해라.\n",
    "\n",
    "- main_menu: 자주 언급되는 메뉴 이름\n",
    "- atmosphere: 매장의 분위기/경험/특징\n",
    "- recommended_for: 어떤 사람이 방문하면 좋을지 (ex. 연인, 친구, 반려견과 함께 등)\n",
    "\n",
    "반드시 키워드 위주의 짧은 표현만 사용해라.\n",
    "\n",
    "리뷰 텍스트:\n",
    "----------------\n",
    "{text}\n",
    "----------------\n",
    "\"\"\")\n",
    "\n",
    "# --- 4) LCEL 체인 ---\n",
    "summarize_chain = prompt | model\n",
    "\n",
    "\n",
    "# --- 5) 실행 예제 ---\n",
    "\n",
    "\n",
    "def extract_review_keywords(input_text: str) -> ReviewExtraction:\n",
    "    \"\"\"\n",
    "    한 가게의 리뷰 전체를 하나의 문자열로 받아서\n",
    "    대표 메뉴 / 분위기 / 추천 대상을 추출한다.\n",
    "    \"\"\"\n",
    "    if not input_text.strip():\n",
    "        # 비어 있으면 그냥 기본값 리턴\n",
    "        return ReviewExtraction(\n",
    "            main_menu=[],\n",
    "            atmosphere=[],\n",
    "            recommended_for=[],\n",
    "        )\n",
    "\n",
    "    result: ReviewExtraction = summarize_chain.invoke({\"text\": input_text})\n",
    "    return result\n",
    "result = extract_review_keywords(input_string)\n",
    "print(result)\n",
    "print(result.main_menu)\n",
    "print(result.atmosphere)\n",
    "print(result.recommended_for)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f76314",
   "metadata": {},
   "source": [
    "# 3개 가게 동시 리뷰 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9822d2a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif __name__ == \"__main__\":\\n    # 단일 가게 테스트용\\n    store = \"카피로우 일산밤리단길카페점\"\\n    df = crawl_naver_blog_reviews(store_name=store, scroll_count=1, max_posts=5)\\n    input_text = build_review_input_text(df[\"only_kor_contents\"])\\n    print(\"리뷰 텍스트 길이:\", len(input_text))'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# crawling.py\n",
    "import re\n",
    "import time\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "\n",
    "def extract_korean(text: str) -> str:\n",
    "    \"\"\"문자열에서 한글만 추출해서 공백으로 이어붙임.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    result = re.compile(\"[가-힣]+\").findall(text)\n",
    "    return \" \".join(result)\n",
    "\n",
    "\n",
    "def build_search_url(store_name: str) -> str:\n",
    "    \"\"\"가게 이름으로 네이버 블로그 검색 URL 생성.\"\"\"\n",
    "    query = quote_plus(store_name)\n",
    "    url = f\"https://search.naver.com/search.naver?ssc=tab.blog.all&sm=tab_jum&query={query}\"\n",
    "    return url\n",
    "\n",
    "\n",
    "def _create_driver() -> webdriver.Chrome:\n",
    "    \"\"\"크롬 드라이버 생성 (속도 최적화 옵션 포함).\"\"\"\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument(\"--headless=new\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "    # 이미지 로딩 끄기 (속도 + 트래픽 감소)\n",
    "    prefs = {\"profile.managed_default_content_settings.images\": 2}\n",
    "    chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.set_window_size(1280, 800)\n",
    "    return driver\n",
    "\n",
    "\n",
    "def crawl_naver_blog_reviews(\n",
    "    store_name: str,\n",
    "    scroll_count: int = 1,\n",
    "    max_posts: int = 5,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    네이버 블로그에서 특정 가게 이름으로 검색한 뒤\n",
    "    블로그 글 제목, 링크, 내용, 한글만 추출한 내용을 크롤링해서 DataFrame 반환.\n",
    "\n",
    "    컬럼: titles, links, contents, only_kor_contents\n",
    "    \"\"\"\n",
    "\n",
    "    driver = _create_driver()\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "\n",
    "    try:\n",
    "        # 1. 검색 페이지 접속\n",
    "        search_url = build_search_url(store_name)\n",
    "        driver.get(search_url)\n",
    "\n",
    "        # 2. 검색 결과 로딩 대기\n",
    "        try:\n",
    "            wait.until(\n",
    "                EC.presence_of_all_elements_located(\n",
    "                    (By.CSS_SELECTOR, \"div.sds-comps-vertical-layout.sds-comps-full-layout\")\n",
    "                )\n",
    "            )\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # 3. 스크롤 (조금만)\n",
    "        body = driver.find_element(By.CSS_SELECTOR, \"body\")\n",
    "        for _ in range(scroll_count):\n",
    "            body.send_keys(Keys.PAGE_DOWN)\n",
    "            time.sleep(0.5)\n",
    "\n",
    "        # 4. HTML 파싱\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        # 5. 블로그 카드에서 제목/링크 추출\n",
    "        url_soup = (\n",
    "            soup.select(\n",
    "                \"div > div.sds-comps-vertical-layout.sds-comps-full-layout.N8JknjVu2Kc8aLavF6ZA > a\"\n",
    "            )\n",
    "            + soup.select(\n",
    "                \"div > div.sds-comps-vertical-layout.sds-comps-full-layout.MkfloTjOr2Rg4LLlLwVA > a\"\n",
    "            )\n",
    "            +soup.select(\n",
    "                'div > div.sds-comps-vertical-layout.sds-comps-full-layout._IJWW1BBVoZsf1hqzdtq > a'\n",
    "                )\n",
    "        )\n",
    "\n",
    "        url_soup = url_soup[:max_posts]\n",
    "\n",
    "        titles = []\n",
    "        links = []\n",
    "\n",
    "        for t in url_soup:\n",
    "            title_text = t.get_text().strip()\n",
    "            link = t.attrs.get(\"href\", \"\").strip()\n",
    "            if not link:\n",
    "                continue\n",
    "            print(\"[BLOG]\", title_text, link)\n",
    "            titles.append(title_text)\n",
    "            links.append(link)\n",
    "\n",
    "        # 6. 각 블로그 글 본문 크롤링\n",
    "        contents = []\n",
    "\n",
    "        for idx, link in enumerate(links):\n",
    "            try:\n",
    "                driver.get(link)\n",
    "\n",
    "                # 본문/iframe 로딩 대기\n",
    "                try:\n",
    "                    wait.until(\n",
    "                        EC.presence_of_element_located(\n",
    "                            (\n",
    "                                By.CSS_SELECTOR,\n",
    "                                \"iframe#mainFrame, div.se-main-container, div#postViewArea\",\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "                # 구버전 블로그(iframe)면 frame 전환\n",
    "                switched = False\n",
    "                try:\n",
    "                    driver.switch_to.frame(\"mainFrame\")\n",
    "                    switched = True\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "                text = \"\"\n",
    "                # 새 에디터\n",
    "                try:\n",
    "                    text = driver.find_element(By.CSS_SELECTOR, \"div.se-main-container\").text\n",
    "                except Exception:\n",
    "                    # 구 에디터\n",
    "                    try:\n",
    "                        text = driver.find_element(By.CSS_SELECTOR, \"div#postViewArea\").text\n",
    "                    except Exception:\n",
    "                        text = \"\"\n",
    "\n",
    "                contents.append(text)\n",
    "\n",
    "                if switched:\n",
    "                    driver.switch_to.default_content()\n",
    "\n",
    "                print(f\"[CONTENT] {idx+1}/{len(links)} 크롤링 완료\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] {link} 크롤링 중 오류: {e}\")\n",
    "                contents.append(\"\")\n",
    "                try:\n",
    "                    driver.switch_to.default_content()\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        # 7. 한글만 남긴 컬럼 생성\n",
    "        only_kor_contents = [extract_korean(c) for c in contents]\n",
    "\n",
    "        # 8. DataFrame 생성\n",
    "        review_data = pd.DataFrame(\n",
    "            {\n",
    "                \"titles\": titles,\n",
    "                \"links\": links,\n",
    "                \"contents\": contents,\n",
    "                \"only_kor_contents\": only_kor_contents,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return review_data\n",
    "\n",
    "    finally:\n",
    "        try:\n",
    "            driver.quit()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "def build_review_input_text(\n",
    "    review_series,\n",
    "    max_reviews: int = 8,\n",
    "    max_chars_per_review: int = 1500,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    DataFrame의 only_kor_contents 컬럼에서\n",
    "    최대 max_reviews개, 글당 max_chars_per_review까지 잘라 합친다.\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    for raw in review_series[:max_reviews]:\n",
    "        if not isinstance(raw, str):\n",
    "            continue\n",
    "        text = raw.strip()\n",
    "        if not text:\n",
    "            continue\n",
    "        text = text[:max_chars_per_review]\n",
    "        texts.append(text)\n",
    "\n",
    "    return \"\\n\\n---\\n\\n\".join(texts)\n",
    "\n",
    "'''\n",
    "if __name__ == \"__main__\":\n",
    "    # 단일 가게 테스트용\n",
    "    store = \"카피로우 일산밤리단길카페점\"\n",
    "    df = crawl_naver_blog_reviews(store_name=store, scroll_count=1, max_posts=5)\n",
    "    input_text = build_review_input_text(df[\"only_kor_contents\"])\n",
    "    print(\"리뷰 텍스트 길이:\", len(input_text))'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee53405e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===  크롤링 시작 ===\n",
      " 리뷰 텍스트 길이: 0\n",
      "\n",
      "=== 몽키 일산 밤리단길카페 본점 크롤링 시작 ===\n",
      "[BLOG] 일산 밤리단길 카페 몽킽｜트리플치즈·모카소금빵·몽키·흑임자슈페너 https://blog.naver.com/diana-0/224034258895\n",
      "[BLOG] 일산 소금빵 맛집 몽킽 일산밤리단길 카페본점 https://blog.naver.com/sunmi9333/224087769839\n",
      "[BLOG] 일산 소금빵 맛집 몽킽 밤리단길 카페 본점 데이트 https://blog.naver.com/denimfrog/224039866074\n",
      "[BLOG] [일산 밤리단길] 소금빵과 몽키슈페너 맛집 몽킽카페_구.베로티오 https://blog.naver.com/saymams/223656434798\n",
      "[BLOG] 일산 밤리단길카페 몽킽 소금빵과 시그니처 음료 몽키슈페너와 함께 즐기는 커피타임 https://blog.naver.com/chlgmldu1028love/223861112954\n",
      "[CONTENT] 1/5 크롤링 완료\n",
      "[CONTENT] 2/5 크롤링 완료\n",
      "[CONTENT] 3/5 크롤링 완료\n",
      "[CONTENT] 4/5 크롤링 완료\n",
      "[CONTENT] 5/5 크롤링 완료\n",
      "몽키 일산 밤리단길카페 본점 리뷰 텍스트 길이: 7528\n",
      "\n",
      "=== 마제야 마제소바 전문점뒷북서재 크롤링 시작 ===\n",
      "[BLOG] 대구 서재세천맛집 한닢 다사세천점 다사 밥집 마제소바 스테이크 덮밥 솔직후기 https://blog.naver.com/smjy74/223358983479\n",
      "[BLOG] 청라 핫플1. 판코 마제소바 후기 https://blog.naver.com/ggami9997/222824596340\n",
      "[BLOG] 김포 구래동 백소정 마제소바 돈까스 맛집 https://blog.naver.com/ississ777/223216517180\n",
      "[BLOG] 대구 다사 맛집 마제소바가 맛있는 한닢라멘 https://blog.naver.com/gjswn61/222657332681\n",
      "[BLOG] [나고야여행] Day 2 나고야항,하브스 케이크,돈키호테,마제소바(쯔케멘혼마루) https://blog.naver.com/kyj3285/223114907533\n",
      "[CONTENT] 1/5 크롤링 완료\n",
      "[CONTENT] 2/5 크롤링 완료\n",
      "[CONTENT] 3/5 크롤링 완료\n",
      "[CONTENT] 4/5 크롤링 완료\n",
      "[CONTENT] 5/5 크롤링 완료\n",
      "마제야 마제소바 전문점뒷북서재 리뷰 텍스트 길이: 6603\n",
      "\n",
      "=== LLM 요약 (3개 가게 batch) 시작 ===\n",
      "\n",
      "########  ########\n",
      "대표 메뉴: ['비빔밥', '김치찌개', '불고기', '된장찌개']\n",
      "분위기: ['아늑한', '전통적인', '편안한', '조용한']\n",
      "추천 대상: ['가족', '연인', '친구', '혼자']\n",
      "\n",
      "######## 몽키 일산 밤리단길카페 본점 ########\n",
      "대표 메뉴: ['소금빵', '트리플치즈소금빵', '모카소금빵', '몽키슈페너']\n",
      "분위기: ['아늑한 분위기', '감각적인 인테리어', '유럽 카페 느낌', '창의적인 공간']\n",
      "추천 대상: ['연인', '가족', '친구', '반려동물 동반 가능']\n",
      "\n",
      "######## 마제야 마제소바 전문점뒷북서재 ########\n",
      "대표 메뉴: ['마제소바', '토시살 스테이크 덮밥', '차슈카츠', '왕새우튀김']\n",
      "분위기: ['깔끔한 인테리어', '포근한 느낌', '유아의자 구비', '친절한 서비스']\n",
      "추천 대상: ['가족', '연인', '친구', '혼자서 식사하는 사람']\n"
     ]
    }
   ],
   "source": [
    "# llm_summarize.py\n",
    "from typing import List, Dict\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "#from crawling import crawl_naver_blog_reviews, build_review_input_text\n",
    "\n",
    "\n",
    "load_dotenv()  # .env에서 OPENAI_API_KEY 로드\n",
    "\n",
    "\n",
    "# --- 1) 구조화된 Output 모델 ---\n",
    "class ReviewExtraction(BaseModel):\n",
    "    main_menu: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"가게에서 많이 언급되는 대표 메뉴 키워드들 (예: 소금빵, 아메리카노, 고구마라떼)\",\n",
    "    )\n",
    "    atmosphere: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"가게 분위기, 경험, 매장 특징 키워드들 (예: 아늑한, 감성적인, 좌석이 넓은)\",\n",
    "    )\n",
    "    recommended_for: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"어떤 유형의 사람이 방문하면 좋은지 (예: 연인과 함께, 친구와 수다, 반려견과 함께)\",\n",
    "    )\n",
    "\n",
    "\n",
    "# --- 2) LLM + 구조화 출력 준비 ---\n",
    "base_model = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\",   # 빠르고 저렴한 모델\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "model = base_model.with_structured_output(ReviewExtraction)\n",
    "\n",
    "\n",
    "# --- 3) 프롬프트 ---\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "너는 한국어 네이버 블로그 리뷰를 분석해서\n",
    "가게의 대표 메뉴, 분위기, 추천 대상을 키워드로만 뽑는 역할을 한다.\n",
    "\n",
    "아래 리뷰 텍스트를 보고,\n",
    "각 항목당 3~4개의 핵심 키워드를 한국어로만 추출해라.\n",
    "\n",
    "- main_menu: 자주 언급되는 메뉴 이름\n",
    "- atmosphere: 매장의 분위기/경험/특징\n",
    "- recommended_for: 어떤 사람이 방문하면 좋을지 (ex. 연인, 친구, 반려견과 함께 등)\n",
    "\n",
    "반드시 키워드 위주의 짧은 표현만 사용해라.\n",
    "\n",
    "리뷰 텍스트:\n",
    "----------------\n",
    "{text}\n",
    "----------------\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# --- 4) LCEL 체인 ---\n",
    "summarize_chain = prompt | model\n",
    "\n",
    "\n",
    "# --- 5) 단일 가게용 함수 ---\n",
    "def extract_review_keywords(input_text: str) -> ReviewExtraction:\n",
    "    \"\"\"\n",
    "    한 가게의 리뷰 전체를 하나의 문자열로 받아서\n",
    "    대표 메뉴 / 분위기 / 추천 대상을 추출한다.\n",
    "    \"\"\"\n",
    "    if not input_text.strip():\n",
    "        return ReviewExtraction(main_menu=[], atmosphere=[], recommended_for=[])\n",
    "\n",
    "    result: ReviewExtraction = summarize_chain.invoke({\"text\": input_text})\n",
    "    return result\n",
    "\n",
    "\n",
    "# --- 6) 여러 가게 batch 처리용 함수 ---\n",
    "def extract_review_keywords_batch(\n",
    "    store_to_text: Dict[str, str]\n",
    ") -> Dict[str, ReviewExtraction]:\n",
    "    \"\"\"\n",
    "    여러 가게의 리뷰 텍스트를 한 번에 LLM에 보내서\n",
    "    {가게이름: ReviewExtraction} 형태로 결과를 반환한다.\n",
    "    \"\"\"\n",
    "    store_names = list(store_to_text.keys())\n",
    "    inputs = []\n",
    "\n",
    "    for store_name in store_names:\n",
    "        text = store_to_text[store_name] or \"\"\n",
    "        text = text.strip()\n",
    "        inputs.append({\"text\": text})\n",
    "\n",
    "    results: List[ReviewExtraction] = summarize_chain.batch(inputs)\n",
    "\n",
    "    store_to_result: Dict[str, ReviewExtraction] = {}\n",
    "    for store_name, res in zip(store_names, results):\n",
    "        if isinstance(res, ReviewExtraction):\n",
    "            store_to_result[store_name] = res\n",
    "        else:\n",
    "            store_to_result[store_name] = ReviewExtraction(\n",
    "                main_menu=[], atmosphere=[], recommended_for=[]\n",
    "            )\n",
    "\n",
    "    return store_to_result\n",
    "\n",
    "\n",
    "# --- 7) 가게 3개를 한 번에: 크롤링부터 요약까지 ---\n",
    "if __name__ == \"__main__\":\n",
    "    stores = [\n",
    "        \"몽키 일산 밤리단길카페 본점\",\n",
    "        \"마제야 마제소바 전문점\"\n",
    "        \"뒷북서재\",\n",
    "    ]\n",
    "\n",
    "    store_to_text: Dict[str, str] = {}\n",
    "\n",
    "    for store in stores:\n",
    "        print(f\"\\n=== {store} 크롤링 시작 ===\")\n",
    "        df = crawl_naver_blog_reviews(store_name=store, scroll_count=1, max_posts=5)\n",
    "        input_text = build_review_input_text(df[\"only_kor_contents\"])\n",
    "        print(f\"{store} 리뷰 텍스트 길이: {len(input_text)}\")\n",
    "        store_to_text[store] = input_text\n",
    "\n",
    "    print(\"\\n=== LLM 요약 (3개 가게 batch) 시작 ===\")\n",
    "    batch_result = extract_review_keywords_batch(store_to_text)\n",
    "\n",
    "    for store, info in batch_result.items():\n",
    "        print(f\"\\n######## {store} ########\")\n",
    "        print(\"대표 메뉴:\", info.main_menu)\n",
    "        print(\"분위기:\", info.atmosphere)\n",
    "        print(\"추천 대상:\", info.recommended_for)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16d52d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAIN] 카피로우 일산밤리단길카페점 크롤링 시작[MAIN] 몽킽 일산 밤리단길카페 본점 크롤링 시작\n",
      "\n",
      "[MAIN] 뒷북서재 크롤링 시작\n",
      "[BLOG] 디저트가 특히 맛있었던 일산 카페, 카피로우 일산밤리단길카페점 방문기 https://blog.naver.com/hidew777/224076747828\n",
      "[BLOG] 카피로우 일산밤리단길카페점 솔직후기 https://blog.naver.com/rhdmsdl0720/223996105928\n",
      "[BLOG] 카피로우 일산밤리단길카페점 내돈내산 솔직후기 https://blog.naver.com/rkguscjswo26/223891656310\n",
      "[BLOG] “카피로우” 일산밤리단길카페점.크렘당쥬?프랑스디저트 천사의크림일산 밤리단길카페추천 https://blog.naver.com/comma2024/223785598110\n",
      "[BLOG] [경기/고양시] 일산동구 마두동 밤리단길 데이트 카페 추천 무화과 디저트 맛집 찾고 있다면? ‘카피로우 일산밤리단길카페점’ - 아메리카노, 무화과요거트케이크(글루텐프리) https://blog.naver.com/vivid_07/224064134098\n",
      "[BLOG] 일산 소금빵 맛집 몽킽 일산밤리단길 카페본점 https://blog.naver.com/sunmi9333/224087769839\n",
      "[BLOG] 일산 소금빵 맛집 몽킽 밤리단길 카페 본점 데이트 https://blog.naver.com/denimfrog/224039866074\n",
      "[BLOG] 밤리단길카페 추천, 몽킽본점 일산디저트 하면 떠오르는 소금빵맛집 https://blog.naver.com/sojan_euni/224087549948\n",
      "[BLOG] 일산 밤리단길 카페 소금빵 맛있는 몽킽 다녀옴 https://blog.naver.com/atti_jj0905/224061508313\n",
      "[BLOG] 밤리단길 카페 추천 일산디저트   몽킽일산밤리단길본점 https://blog.naver.com/22bin_/224096598370\n",
      "[BLOG] 일산 밤리단길 힐링 북카페 뒷북서재 https://blog.naver.com/arosa0206/223327567292\n",
      "[BLOG] 일산 정발산 조용한 북카페 추천 책과 만년필이 있는 뒷북서재 주차정보 https://blog.naver.com/ageeable/223822881724\n",
      "[BLOG] 뒷북서재ㅣ일산 밤리단길 조용한 북 카페를 찾는다면 바로 여기 https://blog.naver.com/minin0702/224033987266\n",
      "[BLOG] [일산] 밤리단길 카페 추천 '뒷북 서재' 내돈내산 후기 https://blog.naver.com/areum9716/224097867816\n",
      "[BLOG] 일산 밤리단길 편안한 북카페 뒷북서재 https://blog.naver.com/traveler_dooly/223305369057\n",
      "[CONTENT] 1/5 크롤링 완료\n",
      "[CONTENT] 2/5 크롤링 완료\n",
      "[CONTENT] 1/5 크롤링 완료\n",
      "[CONTENT] 1/5 크롤링 완료\n",
      "[CONTENT] 3/5 크롤링 완료\n",
      "[CONTENT] 2/5 크롤링 완료\n",
      "[CONTENT] 2/5 크롤링 완료\n",
      "[CONTENT] 4/5 크롤링 완료\n",
      "[CONTENT] 3/5 크롤링 완료\n",
      "[CONTENT] 5/5 크롤링 완료\n",
      "[CONTENT] 4/5 크롤링 완료\n",
      "[CONTENT] 3/5 크롤링 완료\n",
      "[CONTENT] 5/5 크롤링 완료\n",
      "[CONTENT] 4/5 크롤링 완료\n",
      "[MAIN] 카피로우 일산밤리단길카페점 리뷰 텍스트 길이 = 5116\n",
      "[MAIN] 뒷북서재 리뷰 텍스트 길이 = 6277\n",
      "[CONTENT] 5/5 크롤링 완료\n",
      "[MAIN] 몽킽 일산 밤리단길카페 본점 리뷰 텍스트 길이 = 7528\n",
      "\n",
      "===== 카피로우 일산밤리단길카페점 =====\n",
      "텍스트 길이: 5116\n",
      "\n",
      "===== 뒷북서재 =====\n",
      "텍스트 길이: 6277\n",
      "\n",
      "===== 몽킽 일산 밤리단길카페 본점 =====\n",
      "텍스트 길이: 7528\n"
     ]
    }
   ],
   "source": [
    "# parallel_thread_crawl.py\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import Dict\n",
    "\n",
    "#from crawling import crawl_naver_blog_reviews\n",
    "\n",
    "\n",
    "def build_review_input_text(review_series, max_reviews=8, max_chars_per_review=1500) -> str:\n",
    "    texts = []\n",
    "    for raw in review_series[:max_reviews]:\n",
    "        if not isinstance(raw, str):\n",
    "            continue\n",
    "        text = raw.strip()\n",
    "        if not text:\n",
    "            continue\n",
    "        texts.append(text[:max_chars_per_review])\n",
    "    return \"\\n\\n---\\n\\n\".join(texts)\n",
    "\n",
    "\n",
    "def crawl_one_store_to_text(store_name: str) -> tuple[str, str]:\n",
    "    print(f\"[MAIN] {store_name} 크롤링 시작\")\n",
    "    df = crawl_naver_blog_reviews(store_name=store_name, scroll_count=1, max_posts=5)\n",
    "    if df.empty:\n",
    "        print(f\"[MAIN] {store_name} 결과 없음\")\n",
    "        return store_name, \"\"\n",
    "    text = build_review_input_text(df[\"only_kor_contents\"])\n",
    "    print(f\"[MAIN] {store_name} 리뷰 텍스트 길이 = {len(text)}\")\n",
    "    return store_name, text\n",
    "\n",
    "\n",
    "def crawl_stores_in_threads(stores: list[str], max_workers: int = 3) -> Dict[str, str]:\n",
    "    store_to_text: Dict[str, str] = {}\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_store = {\n",
    "            executor.submit(crawl_one_store_to_text, store): store for store in stores\n",
    "        }\n",
    "\n",
    "        for future in as_completed(future_to_store):\n",
    "            store = future_to_store[future]\n",
    "            try:\n",
    "                s_name, text = future.result()\n",
    "                store_to_text[s_name] = text\n",
    "            except Exception as e:\n",
    "                print(f\"[MAIN] {store} 쓰레드에서 예외 발생: {e}\")\n",
    "                store_to_text[store] = \"\"\n",
    "\n",
    "    return store_to_text\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    stores = [\n",
    "        \"카피로우 일산밤리단길카페점\",\n",
    "        \"몽킽 일산 밤리단길카페 본점\",\n",
    "        \"뒷북서재\",\n",
    "    ]\n",
    "\n",
    "    result = crawl_stores_in_threads(stores, max_workers=3)\n",
    "\n",
    "    for store, text in result.items():\n",
    "        print(f\"\\n===== {store} =====\")\n",
    "        print(\"텍스트 길이:\", len(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fef7325",
   "metadata": {},
   "source": [
    "# 스레드 크롤링(3개) + LLM batch 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16ac5a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAIN] 카피로우 일산밤리단길카페점 크롤링 시작\n",
      "[MAIN] 몽키 일산 밤리단길카페 본점 크롤링 시작\n",
      "[MAIN] 뒷북서재 크롤링 시작\n",
      "[몽키 일산 밤리단길카페 본점] BLOG: 내 블로그 https://blog.naver.com/MyBlog.naver\n",
      "[몽키 일산 밤리단길카페 본점] BLOG: 블로그 https://section.blog.naver.com/\n",
      "[몽키 일산 밤리단길카페 본점] BLOG:  https://blog.naver.com/diana-0\n",
      "[몽키 일산 밤리단길카페 본점] BLOG: 태평한냠뉴 https://blog.naver.com/diana-0\n",
      "[몽키 일산 밤리단길카페 본점] BLOG: 일산 밤리단길 카페 몽킽｜트리플치즈·모카소금빵·몽키·흑임자슈페너 https://blog.naver.com/diana-0/224034258895\n",
      "[INFO] 몽키 일산 밤리단길카페 본점 링크 개수: 5\n",
      "[카피로우 일산밤리단길카페점] BLOG: 내 블로그 https://blog.naver.com/MyBlog.naver\n",
      "[카피로우 일산밤리단길카페점] BLOG: 블로그 https://section.blog.naver.com/\n",
      "[카피로우 일산밤리단길카페점] BLOG:  https://blog.naver.com/hidew777\n",
      "[카피로우 일산밤리단길카페점] BLOG: 행복연구 리뷰메이커의 맛집, 리뷰, 일상 기록 블로그 https://blog.naver.com/hidew777\n",
      "[카피로우 일산밤리단길카페점] BLOG: 디저트가 특히 맛있었던 일산 카페, 카피로우 일산밤리단길카페점 방문기 https://blog.naver.com/hidew777/224076747828\n",
      "[INFO] 카피로우 일산밤리단길카페점 링크 개수: 5\n",
      "[몽키 일산 밤리단길카페 본점] CONTENT 1/5 len=0\n",
      "[뒷북서재] BLOG: 내 블로그 https://blog.naver.com/MyBlog.naver\n",
      "[뒷북서재] BLOG: 블로그 https://section.blog.naver.com/\n",
      "[뒷북서재] BLOG:  https://blog.naver.com/arosa0206\n",
      "[뒷북서재] BLOG: 로사의 맛집탐방 https://blog.naver.com/arosa0206\n",
      "[뒷북서재] BLOG: 일산 밤리단길 힐링 북카페 뒷북서재 https://blog.naver.com/arosa0206/223327567292\n",
      "[INFO] 뒷북서재 링크 개수: 5\n",
      "[카피로우 일산밤리단길카페점] CONTENT 1/5 len=0\n",
      "[몽키 일산 밤리단길카페 본점] CONTENT 2/5 len=0\n",
      "[뒷북서재] CONTENT 1/5 len=0\n",
      "[카피로우 일산밤리단길카페점] CONTENT 2/5 len=0\n",
      "[몽키 일산 밤리단길카페 본점] CONTENT 3/5 len=0\n",
      "[뒷북서재] CONTENT 2/5 len=0\n",
      "[카피로우 일산밤리단길카페점] CONTENT 3/5 len=1890\n",
      "[뒷북서재] CONTENT 3/5 len=0\n",
      "[몽키 일산 밤리단길카페 본점] CONTENT 4/5 len=0\n",
      "[카피로우 일산밤리단길카페점] CONTENT 4/5 len=1890\n",
      "[뒷북서재] CONTENT 4/5 len=0\n",
      "[몽키 일산 밤리단길카페 본점] CONTENT 5/5 len=2643\n",
      "[카피로우 일산밤리단길카페점] CONTENT 5/5 len=3593\n",
      "[MAIN] 몽키 일산 밤리단길카페 본점: 리뷰 텍스트 길이 = 1500\n",
      "[MAIN] 카피로우 일산밤리단길카페점: 리뷰 텍스트 길이 = 4514\n",
      "[뒷북서재] CONTENT 5/5 len=2317\n",
      "[MAIN] 뒷북서재: 리뷰 텍스트 길이 = 1500\n",
      "\n",
      "===== 몽키 일산 밤리단길카페 본점 =====\n",
      "텍스트 길이: 1500\n",
      "\n",
      "===== 카피로우 일산밤리단길카페점 =====\n",
      "텍스트 길이: 4514\n",
      "\n",
      "===== 뒷북서재 =====\n",
      "텍스트 길이: 1500\n"
     ]
    }
   ],
   "source": [
    "# crawling.py\n",
    "import re\n",
    "import time\n",
    "from urllib.parse import quote_plus\n",
    "from typing import Dict, List\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "def extract_korean(text: str) -> str:\n",
    "    \"\"\"문자열에서 한글만 추출해서 공백으로 이어붙임.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    result = re.compile(\"[가-힣]+\").findall(text)\n",
    "    return \" \".join(result)\n",
    "\n",
    "\n",
    "def build_search_url(store_name: str) -> str:\n",
    "    \"\"\"가게 이름으로 네이버 블로그 검색 URL 생성.\"\"\"\n",
    "    query = quote_plus(store_name)\n",
    "    return f\"https://search.naver.com/search.naver?ssc=tab.blog.all&sm=tab_jum&query={query}\"\n",
    "\n",
    "\n",
    "def _create_driver() -> webdriver.Chrome:\n",
    "    \"\"\"단일 크롤링용 Chrome 드라이버 생성.\"\"\"\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    # 디버깅할 땐 아래 줄 주석 처리하고 실제 창 보면서 하면 좋음\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "\n",
    "def crawl_naver_blog_reviews(\n",
    "    store_name: str,\n",
    "    scroll_count: int = 1,\n",
    "    max_posts: int = 5,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    네이버 블로그에서 특정 가게 이름으로 검색한 뒤\n",
    "    블로그 글 제목, 링크, 내용, 한글만 추출한 내용을 크롤링해서 DataFrame 반환.\n",
    "\n",
    "    컬럼: titles, links, contents, only_kor_contents\n",
    "    \"\"\"\n",
    "    driver = _create_driver()\n",
    "\n",
    "    try:\n",
    "        search_url = build_search_url(store_name)\n",
    "        driver.get(search_url)\n",
    "        time.sleep(3)\n",
    "\n",
    "        body = driver.find_element(By.CSS_SELECTOR, \"body\")\n",
    "        for _ in range(scroll_count):\n",
    "            body.send_keys(Keys.PAGE_DOWN)\n",
    "            time.sleep(2)\n",
    "\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        # 클래스 대신 blog.naver.com 도메인 기준으로 링크 찾기\n",
    "        url_soup = soup.select('a[href*=\"blog.naver.com\"]')\n",
    "\n",
    "        titles: List[str] = []\n",
    "        links: List[str] = []\n",
    "\n",
    "        for t in url_soup[:max_posts]:\n",
    "            title_text = t.get_text().strip()\n",
    "            link = t.get(\"href\", \"\").strip()\n",
    "            if not link:\n",
    "                continue\n",
    "\n",
    "            print(f\"[{store_name}] BLOG:\", title_text, link)\n",
    "            titles.append(title_text)\n",
    "            links.append(link)\n",
    "\n",
    "        print(f\"[INFO] {store_name} 링크 개수: {len(links)}\")\n",
    "\n",
    "        contents: List[str] = []\n",
    "\n",
    "        for idx, link in enumerate(links):\n",
    "            try:\n",
    "                driver.get(link)\n",
    "                time.sleep(2)\n",
    "\n",
    "                # 구버전 블로그 iframe 시도\n",
    "                try:\n",
    "                    driver.switch_to.frame(\"mainFrame\")\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "                text = \"\"\n",
    "                # 새 에디터\n",
    "                try:\n",
    "                    text = driver.find_element(By.CSS_SELECTOR, \"div.se-main-container\").text\n",
    "                except Exception:\n",
    "                    # 구 에디터\n",
    "                    try:\n",
    "                        text = driver.find_element(By.CSS_SELECTOR, \"div#postViewArea\").text\n",
    "                    except Exception:\n",
    "                        text = \"\"\n",
    "\n",
    "                contents.append(text)\n",
    "                print(f\"[{store_name}] CONTENT {idx+1}/{len(links)} len={len(text)}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[{store_name}] ERROR {link}: {e}\")\n",
    "                contents.append(\"\")\n",
    "            finally:\n",
    "                try:\n",
    "                    driver.switch_to.default_content()\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        only_kor_contents = [extract_korean(c) for c in contents]\n",
    "\n",
    "        return pd.DataFrame(\n",
    "            {\n",
    "                \"titles\": titles,\n",
    "                \"links\": links,\n",
    "                \"contents\": contents,\n",
    "                \"only_kor_contents\": only_kor_contents,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "def build_review_input_text(\n",
    "    review_series,\n",
    "    max_reviews: int = 8,\n",
    "    max_chars_per_review: int = 1500,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    DataFrame의 only_kor_contents에서\n",
    "    최대 max_reviews개, 글당 max_chars_per_review까지 잘라 합침.\n",
    "    \"\"\"\n",
    "    texts: List[str] = []\n",
    "    for raw in review_series[:max_reviews]:\n",
    "        if not isinstance(raw, str):\n",
    "            continue\n",
    "        text = raw.strip()\n",
    "        if not text:\n",
    "            continue\n",
    "        texts.append(text[:max_chars_per_review])\n",
    "    return \"\\n\\n---\\n\\n\".join(texts)\n",
    "\n",
    "\n",
    "def crawl_one_store_to_text(store_name: str) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    스레드에서 실행될 함수.\n",
    "    가게 하나 크롤링 + 리뷰 합친 텍스트까지 반환.\n",
    "    \"\"\"\n",
    "    print(f\"[MAIN] {store_name} 크롤링 시작\")\n",
    "    df = crawl_naver_blog_reviews(store_name=store_name, scroll_count=1, max_posts=5)\n",
    "\n",
    "    if df.empty:\n",
    "        print(f\"[MAIN] {store_name}: 크롤링 결과 없음\")\n",
    "        return store_name, \"\"\n",
    "\n",
    "    text = build_review_input_text(df[\"only_kor_contents\"])\n",
    "    print(f\"[MAIN] {store_name}: 리뷰 텍스트 길이 = {len(text)}\")\n",
    "    return store_name, text\n",
    "\n",
    "\n",
    "def crawl_stores_in_threads(\n",
    "    stores: List[str],\n",
    "    max_workers: int = 3,\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    여러 가게를 스레드로 병렬 크롤링.\n",
    "    각 가게마다 Chrome 드라이버 하나씩 생성해서 사용.\n",
    "\n",
    "    return: {가게이름: 리뷰합친텍스트}\n",
    "    \"\"\"\n",
    "    store_to_text: Dict[str, str] = {}\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_store = {\n",
    "            executor.submit(crawl_one_store_to_text, store): store for store in stores\n",
    "        }\n",
    "\n",
    "        for future in as_completed(future_to_store):\n",
    "            store = future_to_store[future]\n",
    "            try:\n",
    "                s_name, text = future.result()\n",
    "                store_to_text[s_name] = text\n",
    "            except Exception as e:\n",
    "                print(f\"[MAIN] {store} 쓰레드 예외: {e}\")\n",
    "                store_to_text[store] = \"\"\n",
    "\n",
    "    return store_to_text\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 단독 테스트용\n",
    "    stores = [\n",
    "        \"카피로우 일산밤리단길카페점\",\n",
    "        \"몽키 일산 밤리단길카페 본점\",\n",
    "        \"뒷북서재\",\n",
    "    ]\n",
    "\n",
    "    result = crawl_stores_in_threads(stores, max_workers=3)\n",
    "\n",
    "    for s, txt in result.items():\n",
    "        print(f\"\\n===== {s} =====\")\n",
    "        print(\"텍스트 길이:\", len(txt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8556e332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAIN] 카피로우 일산밤리단길카페점 크롤링 시작\n",
      "[MAIN] 몽키 일산 밤리단길카페 본점 크롤링 시작\n",
      "[MAIN] 어쩌구 카페 홍대점 크롤링 시작\n",
      "[어쩌구 카페 홍대점] BLOG: 내 블로그 https://blog.naver.com/MyBlog.naver\n",
      "[어쩌구 카페 홍대점] BLOG: 블로그 https://section.blog.naver.com/\n",
      "[어쩌구 카페 홍대점] BLOG:  https://blog.naver.com/my_day-to-day\n",
      "[어쩌구 카페 홍대점] BLOG: 덕질하는 날들 https://blog.naver.com/my_day-to-day\n",
      "[어쩌구 카페 홍대점] BLOG: 실바니안 아기 셰프의 다락방 카페 엔제리너스 L7홍대점 페르시안 고양이 데려오기! https://blog.naver.com/my_day-to-day/223969460076\n",
      "[INFO] 어쩌구 카페 홍대점 링크 개수: 5\n",
      "[카피로우 일산밤리단길카페점] BLOG: 내 블로그 https://blog.naver.com/MyBlog.naver\n",
      "[카피로우 일산밤리단길카페점] BLOG: 블로그 https://section.blog.naver.com/\n",
      "[카피로우 일산밤리단길카페점] BLOG:  https://blog.naver.com/hidew777\n",
      "[카피로우 일산밤리단길카페점] BLOG: 행복연구 리뷰메이커의 맛집, 리뷰, 일상 기록 블로그 https://blog.naver.com/hidew777\n",
      "[카피로우 일산밤리단길카페점] BLOG: 디저트가 특히 맛있었던 일산 카페, 카피로우 일산밤리단길카페점 방문기 https://blog.naver.com/hidew777/224076747828\n",
      "[INFO] 카피로우 일산밤리단길카페점 링크 개수: 5\n",
      "[몽키 일산 밤리단길카페 본점] BLOG: 내 블로그 https://blog.naver.com/MyBlog.naver\n",
      "[몽키 일산 밤리단길카페 본점] BLOG: 블로그 https://section.blog.naver.com/\n",
      "[몽키 일산 밤리단길카페 본점] BLOG:  https://blog.naver.com/diana-0\n",
      "[몽키 일산 밤리단길카페 본점] BLOG: 태평한냠뉴 https://blog.naver.com/diana-0\n",
      "[몽키 일산 밤리단길카페 본점] BLOG: 일산 밤리단길 카페 몽킽｜트리플치즈·모카소금빵·몽키·흑임자슈페너 https://blog.naver.com/diana-0/224034258895\n",
      "[INFO] 몽키 일산 밤리단길카페 본점 링크 개수: 5\n",
      "[어쩌구 카페 홍대점] CONTENT 1/5 len=0\n",
      "[몽키 일산 밤리단길카페 본점] CONTENT 1/5 len=0\n",
      "[카피로우 일산밤리단길카페점] CONTENT 1/5 len=0\n",
      "[어쩌구 카페 홍대점] CONTENT 2/5 len=0\n",
      "[카피로우 일산밤리단길카페점] CONTENT 2/5 len=0\n",
      "[몽키 일산 밤리단길카페 본점] CONTENT 2/5 len=0\n",
      "[어쩌구 카페 홍대점] CONTENT 3/5 len=1091\n",
      "[카피로우 일산밤리단길카페점] CONTENT 3/5 len=1890\n",
      "[몽키 일산 밤리단길카페 본점] CONTENT 3/5 len=0\n",
      "[어쩌구 카페 홍대점] CONTENT 4/5 len=1091\n",
      "[카피로우 일산밤리단길카페점] CONTENT 4/5 len=1890\n",
      "[어쩌구 카페 홍대점] CONTENT 5/5 len=1298\n",
      "[몽키 일산 밤리단길카페 본점] CONTENT 4/5 len=0\n",
      "[MAIN] 어쩌구 카페 홍대점: 리뷰 텍스트 길이 = 3125\n",
      "[카피로우 일산밤리단길카페점] CONTENT 5/5 len=3593\n",
      "[몽키 일산 밤리단길카페 본점] CONTENT 5/5 len=2643\n",
      "[MAIN] 카피로우 일산밤리단길카페점: 리뷰 텍스트 길이 = 4514\n",
      "[MAIN] 몽키 일산 밤리단길카페 본점: 리뷰 텍스트 길이 = 1500\n",
      "\n",
      "=== LLM 요약 (batch) 시작 ===\n",
      "\n",
      "######## 어쩌구 카페 홍대점 ########\n",
      "대표 메뉴: ['군밤모자', '야구세트', '뽀글이세트', '크리스마스 산타 인형 옷 세트']\n",
      "분위기: ['귀여운', '한정판', '인형 꾸미기', '포토존']\n",
      "추천 대상: ['인형 애호가', '가족', '친구', '덕후']\n",
      "\n",
      "######## 카피로우 일산밤리단길카페점 ########\n",
      "대표 메뉴: ['카페라떼', '땅콩크림라떼', '단호박케이크', '디저트']\n",
      "분위기: ['따뜻하고 아늑한', '조용한 대화 소리', '편안한 분위기', '자연스러운 식물 배치']\n",
      "추천 대상: ['연인', '친구', '반려견과 함께', '디저트 애호가']\n",
      "\n",
      "######## 몽키 일산 밤리단길카페 본점 ########\n",
      "대표 메뉴: ['소금빵', '트리플치즈소금빵', '모카소금빵', '몽키 흑임자 슈페너']\n",
      "분위기: ['감성적인 분위기', '유럽 카페 느낌', '세련된 인테리어', '창의적이고 따뜻한 공간']\n",
      "추천 대상: ['연인', '친구', '혼자', '가족']\n"
     ]
    }
   ],
   "source": [
    "# llm_summarize.py\n",
    "from typing import List, Dict\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from crawling import crawl_stores_in_threads\n",
    "\n",
    "\n",
    "load_dotenv()  # .env에서 OPENAI_API_KEY 로드\n",
    "\n",
    "\n",
    "# --- 1) 구조화된 Output 모델 ---\n",
    "class ReviewExtraction(BaseModel):\n",
    "    main_menu: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"가게에서 많이 언급되는 대표 메뉴 키워드들 (예: 소금빵, 아메리카노, 고구마라떼)\",\n",
    "    )\n",
    "    atmosphere: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"가게 분위기, 경험, 매장 특징 키워드들 (예: 아늑한, 감성적인, 좌석이 넓은)\",\n",
    "    )\n",
    "    recommended_for: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"어떤 유형의 사람이 방문하면 좋은지 (예: 연인과 함께, 친구와 수다, 반려견과 함께)\",\n",
    "    )\n",
    "\n",
    "\n",
    "# --- 2) LLM + 구조화 출력 준비 ---\n",
    "base_model = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\",        # 빠르고 저렴한 모델\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "model = base_model.with_structured_output(ReviewExtraction)\n",
    "\n",
    "\n",
    "# --- 3) 프롬프트 ---\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "너는 한국어 네이버 블로그 리뷰를 분석해서\n",
    "가게의 대표 메뉴, 분위기, 추천 대상을 키워드로만 뽑는 역할을 한다.\n",
    "\n",
    "아래 리뷰 텍스트를 보고,\n",
    "각 항목당 3~4개의 핵심 키워드를 한국어로만 추출해라.\n",
    "\n",
    "- main_menu: 자주 언급되는 메뉴 이름\n",
    "- atmosphere: 매장의 분위기/경험/특징\n",
    "- recommended_for: 어떤 사람이 방문하면 좋을지 (ex. 연인, 친구, 반려견과 함께 등)\n",
    "\n",
    "반드시 키워드 위주의 짧은 표현만 사용해라.\n",
    "\n",
    "리뷰 텍스트:\n",
    "----------------\n",
    "{text}\n",
    "----------------\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# --- 4) LCEL 체인 ---\n",
    "summarize_chain = prompt | model\n",
    "\n",
    "\n",
    "# --- 5) 단일 가게용 함수 ---\n",
    "def extract_review_keywords(input_text: str) -> ReviewExtraction:\n",
    "    if not input_text.strip():\n",
    "        return ReviewExtraction(main_menu=[], atmosphere=[], recommended_for=[])\n",
    "    result: ReviewExtraction = summarize_chain.invoke({\"text\": input_text})\n",
    "    return result\n",
    "\n",
    "\n",
    "# --- 6) 여러 가게 batch 처리 ---\n",
    "def extract_review_keywords_batch(\n",
    "    store_to_text: Dict[str, str]\n",
    ") -> Dict[str, ReviewExtraction]:\n",
    "    store_names = list(store_to_text.keys())\n",
    "    inputs = [{\"text\": store_to_text[name]} for name in store_names]\n",
    "\n",
    "    results: List[ReviewExtraction] = summarize_chain.batch(inputs)\n",
    "\n",
    "    store_to_result: Dict[str, ReviewExtraction] = {}\n",
    "    for name, res in zip(store_names, results):\n",
    "        if isinstance(res, ReviewExtraction):\n",
    "            store_to_result[name] = res\n",
    "        else:\n",
    "            store_to_result[name] = ReviewExtraction(\n",
    "                main_menu=[], atmosphere=[], recommended_for=[]\n",
    "            )\n",
    "\n",
    "    return store_to_result\n",
    "\n",
    "\n",
    "# --- 7) 전체 파이프라인: 3개 가게 크롤링 + 요약 ---\n",
    "if __name__ == \"__main__\":\n",
    "    stores = [\n",
    "        \"카피로우 일산밤리단길카페점\",\n",
    "        \"몽키 일산 밤리단길카페 본점\",\n",
    "        \"뒷북서재\",\n",
    "    ]\n",
    "\n",
    "    # 1) 병렬 크롤링 (가게당 Chrome 하나, ThreadPool)\n",
    "    store_to_text = crawl_stores_in_threads(stores, max_workers=3)\n",
    "\n",
    "    # 2) LLM batch 요약\n",
    "    print(\"\\n=== LLM 요약 (batch) 시작 ===\")\n",
    "    batch_result = extract_review_keywords_batch(store_to_text)\n",
    "\n",
    "    # 3) 결과 출력\n",
    "    for store, info in batch_result.items():\n",
    "        print(f\"\\n######## {store} ########\")\n",
    "        print(\"대표 메뉴:\", info.main_menu)\n",
    "        print(\"분위기:\", info.atmosphere)\n",
    "        print(\"추천 대상:\", info.recommended_for)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
